{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import seaborn as sns\n",
    "from statistics import median\n",
    "import os\n",
    "from os import path\n",
    "sns.set()\n",
    "rng = default_rng()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numberOfSamples = 10000\n",
    "filename = \"microbenchRegressionAll.csv\"\n",
    "outFolderName = \"bootstrapping_\" + str(numberOfSamples)\n",
    "\n",
    "debug = False\n",
    "# iterations of microbenchmarks\n",
    "numberOfIterations = 5\n",
    "CIsmall = 1\n",
    "CImed = 5\n",
    "CIlarge = 10\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! mkdir -p {outFolderName} # type: ignore"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def resample(perfRuntimes1: pd.DataFrame, \n",
    "            perfRuntimes2: pd.DataFrame, \n",
    "            instanceRuns: np.ndarray, \n",
    "            suiteRuns: np.ndarray,\n",
    "            numberOfIterations: int,\n",
    "            numberOfSamples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Resamples performances using hierarchical bootstrapping for building confindence intervals\n",
    "\n",
    "        Builds a tensor of random indices of a form numberOfSamples * instanceRunsNumber * suiteRunsNumber * numberOfIterations.\n",
    "        Then uses these indices to choose from a performance runtimes tensor with a form instanceRunsNumber * suiteRunsNumber * numberOfIterations.\n",
    "        Finally, reshapes resulting tensor to a matrix of form numberOfSamples * ( instanceRunsNumber * suiteRunsNumber * numberOfIterations)\n",
    "        and calculates performance differences between medians.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfRuntimes1 : performance runtimes of the first version.\n",
    "        perfRuntimes2 : performance runtimes of the second version.\n",
    "        instanceRuns : array of instanceRun numbers\n",
    "        suiteRuns : array of suiteRun numbers\n",
    "        numberOfIterations : number of microbenchmark iterations\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            array of performance differences with a shape numberOfSample * 1.\n",
    "\n",
    "        \"\"\"\n",
    "    instanceRunsNumber = instanceRuns.shape[0]\n",
    "    suiteRunsNumber = suiteRuns.shape[0]\n",
    "\n",
    "    allRuntimes1 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "    allRuntimes2 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "    \n",
    "\n",
    "    for instanceRun in instanceRuns:\n",
    "        for suiteRun in suiteRuns:\n",
    "            allRuntimes1[instanceRun - 1][suiteRun] = perfRuntimes1.loc[(perfRuntimes1['instanceRun'] == instanceRun)\n",
    "                                                    & (perfRuntimes1['suiteRun'] == suiteRun),\n",
    "                                                    'runtime'].to_numpy()\n",
    "            allRuntimes2[instanceRun - 1][suiteRun] = perfRuntimes2.loc[(perfRuntimes2['instanceRun'] == instanceRun)\n",
    "                                                    & (perfRuntimes2['suiteRun'] == suiteRun),\n",
    "                                                    'runtime'].to_numpy()\n",
    "\n",
    "    currentInstanceRun = rng.choice(instanceRuns, size=(instanceRunsNumber, numberOfSamples)) - 1\n",
    "    currentSuiteRun = rng.choice(suiteRuns, size=(suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "    currentRuntimes1 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "    currentRuntimes2 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "\n",
    "    tmp1 = allRuntimes1[currentInstanceRun, currentSuiteRun, currentRuntimes1]\n",
    "    tmp1 = np.stack(tmp1, axis=3).reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "\n",
    "    tmp2 = allRuntimes2[currentInstanceRun, currentSuiteRun, currentRuntimes2]\n",
    "    tmp2 = np.stack(tmp2, axis=3).reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "    # Get median for both lists\n",
    "    med1 = np.median(tmp1, axis=1)\n",
    "    med2 = np.median(tmp2, axis=1)\n",
    "    \n",
    "    return med2/med1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_all = pd.read_csv(filename,sep=\";\")\n",
    "\n",
    "\n",
    "# For each microbenchmark\n",
    "for name in df_all.name.unique():\n",
    "    print(f\"Running analysis for benchmark {name}...\")\n",
    "\n",
    "    benchmarkMeasurements = df_all.loc[(df_all['name'].str.startswith(name, na=False))]\n",
    "\n",
    "    instanceRuns = benchmarkMeasurements.instanceRun.unique()\n",
    "    suiteRuns = benchmarkMeasurements.suiteRun.unique()\n",
    "\n",
    "\n",
    "    if len(benchmarkMeasurements) > 300:\n",
    "        printName = name.replace(\"/\",\"-\")\n",
    "        printName = printName.replace(\"/\",\"-\")\n",
    "        benchmarkFilename = os.path.join(outFolderName, printName + \".csv\")\n",
    "        if path.exists(benchmarkFilename):\n",
    "            print(\"  Already analyzed, skip.\")\n",
    "        else:\n",
    "            results = []\n",
    "            # For each commit (number)\n",
    "            for commitNumber in benchmarkMeasurements.number.unique():\n",
    "                print(f\"  Running analysis for commit {commitNumber}...\")\n",
    "\n",
    "                #Find median perf. change\n",
    "                perfRuntimes1 = benchmarkMeasurements.loc[(benchmarkMeasurements['number'] == commitNumber)\n",
    "                                                    & (benchmarkMeasurements['version'] == 1)]\n",
    "                perfRuntimes2 = benchmarkMeasurements.loc[(benchmarkMeasurements['number'] == commitNumber)\n",
    "                                                    & (benchmarkMeasurements['version'] == 2)]\n",
    "                \n",
    "\n",
    "                elements1 = perfRuntimes1['runtime'].shape[0]\n",
    "                elements2 = perfRuntimes2['runtime'].shape[0]\n",
    "                if (elements1 == 45) & (elements2 == 45):\n",
    "\n",
    "                    perf1 = perfRuntimes1['runtime'].median()\n",
    "                    perf2 = perfRuntimes2['runtime'].median()\n",
    "                    # Compare both (e.g., 10ms in ver1 and 12ms in ver2 => 12/10 = 1.2 (>1 -> regression)\n",
    "                    perfChange = ((perf2/perf1) - 1) * 100\n",
    "\n",
    "                    print(f\"    Found median performance change ({perfChange}).\")\n",
    "\n",
    "                    # Run Bootstrapping\n",
    "                    # R stores the 10.000 median values\n",
    "                    R = resample(perfRuntimes1=perfRuntimes1, \n",
    "                            perfRuntimes2=perfRuntimes2, \n",
    "                            instanceRuns=instanceRuns, \n",
    "                            suiteRuns=suiteRuns,\n",
    "                            numberOfIterations=numberOfIterations,\n",
    "                            numberOfSamples=numberOfSamples)\n",
    "\n",
    "                    print(f\"    Bootstrapping done ({len(R)} elements in R).\")\n",
    "\n",
    "                    # Find conf. intervals\n",
    "                    R.sort()\n",
    "\n",
    "                    small = int((numberOfSamples * CIsmall) / 100 / 2)\n",
    "                    if small == 0:\n",
    "                        small  = 1\n",
    "                    medium = int((numberOfSamples * CImed) / 100 / 2)\n",
    "                    large = int((numberOfSamples * CIlarge) / 100 / 2)\n",
    "\n",
    "                    minSmall = R[small-1]\n",
    "                    minSmall = (minSmall - 1) * 100\n",
    "                    maxSmall = R[numberOfSamples-small-1]\n",
    "                    maxSmall = (maxSmall - 1) * 100\n",
    "\n",
    "                    minMedium = R[medium-1]\n",
    "                    minMedium = (minMedium - 1) * 100\n",
    "                    maxMedium = R[numberOfSamples-medium-1]\n",
    "                    maxMedium = (maxMedium - 1) * 100\n",
    "\n",
    "                    minLarge = R[large-1]\n",
    "                    minLarge = (minLarge - 1) * 100\n",
    "                    maxLarge = R[numberOfSamples-large-1]\n",
    "                    maxLarge = (maxLarge - 1) * 100\n",
    "\n",
    "                    print(f\"    Found conf. intervals ([{minSmall}, {maxSmall}],[{minMedium}, {maxMedium}],[{minLarge}, {maxLarge}]).\")\n",
    "\n",
    "                    #Store values\n",
    "                    results.append({\n",
    "                                    \"name\" : name,\n",
    "                                    \"number\" : int(commitNumber),\n",
    "                                    \"medianPerfChange\" : perfChange,\n",
    "                                    \"minSmall\" : minSmall,\n",
    "                                    \"maxSmall\" : maxSmall,\n",
    "                                    \"minMedium\" :minMedium,\n",
    "                                    \"maxMedium\" : maxMedium,\n",
    "                                    \"minLarge\" : minLarge,\n",
    "                                    \"maxLarge\" : maxLarge,\n",
    "                                    })\n",
    "                    print(\"    Values stored.\")\n",
    "                else:\n",
    "                    print(f\"  Skip {name};{commitNumber}, only ({elements1}, {elements2}) elements.\")\n",
    "\n",
    "            #Store results for this benchmark to file\n",
    "            # Convert to data frame\n",
    "            df_results = pd.DataFrame(results)\n",
    "            if len(results) > 0:\n",
    "                df_results.sort_values(by=[\"name\",\"number\"], inplace=True)\n",
    "                df_results.describe()\n",
    "\n",
    "                #Create folder\n",
    "                #Save file\n",
    "                df_results.to_csv(benchmarkFilename, sep=\";\")\n",
    "\n",
    "    else:\n",
    "        print(f\"  Skip {name}, only {len(benchmarkMeasurements)} measurments.\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Done.\")\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "d66d6b08fe58b329152bac548c9eade7eaa4839341ff9c060dccc73094c59e94"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}