{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import seaborn as sns\n",
    "import os\n",
    "from os import path\n",
    "sns.set()\n",
    "rng = default_rng()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "numberOfSamples = 10000\n",
    "#filename = \"../../results_all/micro_history_full_influxdb.csv\"\n",
    "#outFolderName = \"../../results_all/micro_bootstrapping_influx\"\n",
    "\n",
    "filename = \"../../results_all/micro_history_opti_influxdb.csv\"\n",
    "outFolderName = \"../../results_all/micro_bootstrapping_influx_opti\"\n",
    "\n",
    "debug = False\n",
    "slidingAvg=10\n",
    "jumpThreshold = 4\n",
    "trendThreshold = 2\n",
    "\n",
    "\n",
    "# iterations of microbenchmarks\n",
    "numberOfIterations = 5\n",
    "CIsmall = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#! mkdir -p {outFolderName} # type: ignore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def resample(perfRuntimes1: pd.DataFrame,\n",
    "            perfRuntimes2: pd.DataFrame,\n",
    "            instanceRuns: np.ndarray,\n",
    "            suiteRuns: np.ndarray,\n",
    "            numberOfIterations: int,\n",
    "            numberOfSamples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Resamples performances using hierarchical bootstrapping for building confindence intervals\n",
    "\n",
    "        Builds a tensor of random indices of a form numberOfSamples * instanceRunsNumber * suiteRunsNumber * numberOfIterations.\n",
    "        Then uses these indices to choose from a performance runtimes tensor with a form instanceRunsNumber * suiteRunsNumber * numberOfIterations.\n",
    "        Finally, reshapes resulting tensor to a matrix of form numberOfSamples * ( instanceRunsNumber * suiteRunsNumber * numberOfIterations)\n",
    "        and calculates performance differences between medians.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfRuntimes1 : performance runtimes of the first version.\n",
    "        perfRuntimes2 : performance runtimes of the second version.\n",
    "        instanceRuns : array of instanceRun numbers\n",
    "        suiteRuns : array of suiteRun numbers\n",
    "        numberOfIterations : number of microbenchmark iterations\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            array of performance differences with a shape numberOfSample * 1.\n",
    "\n",
    "        \"\"\"\n",
    "    instanceRunsNumber = instanceRuns.shape[0]\n",
    "    suiteRunsNumber = suiteRuns.shape[0]\n",
    "\n",
    "    allRuntimes1 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "    allRuntimes2 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "\n",
    "    #aggregate measurements from all instance and suite runs\n",
    "    for instanceRun in instanceRuns:\n",
    "        for suiteRun in suiteRuns:\n",
    "            allRuntimes1[instanceRun - 1][suiteRun] = perfRuntimes1.loc[(perfRuntimes1['instanceRun'] == instanceRun)\n",
    "                                                    & (perfRuntimes1['suiteRun'] == suiteRun),\n",
    "                                                    'runtime'].to_numpy()\n",
    "            allRuntimes2[instanceRun - 1][suiteRun] = perfRuntimes2.loc[(perfRuntimes2['instanceRun'] == instanceRun)\n",
    "                                                    & (perfRuntimes2['suiteRun'] == suiteRun),\n",
    "                                                    'runtime'].to_numpy()\n",
    "    #Generate Random Arrays\n",
    "    currentInstanceRun = rng.choice(instanceRuns, size=(instanceRunsNumber, numberOfSamples)) - 1\n",
    "    currentSuiteRun = rng.choice(suiteRuns, size=(suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "    currentRuntimes1 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "    currentRuntimes2 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "\n",
    "    #Bulk selection\n",
    "    tmp1 = allRuntimes1[currentInstanceRun, currentSuiteRun, currentRuntimes1]\n",
    "    tmp1 = tmp1.reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "\n",
    "    tmp2 = allRuntimes2[currentInstanceRun, currentSuiteRun, currentRuntimes2]\n",
    "    tmp2 = tmp2.reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "    # Get median for both lists\n",
    "    med1 = np.median(tmp1, axis=1)\n",
    "    med2 = np.median(tmp2, axis=1)\n",
    "\n",
    "    return med2/med1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_1-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_2-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_3-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_4-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_5-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeFloatArrayBlock/1000-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeFloatArrayBlock/5-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeFloatArrayBlock/55-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeFloatArrayBlock/555-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkIndex_IndexFile_TagValueSeriesIDIterator/78888_series_TagValueSeriesIDIterator/cache-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkIndex_IndexFile_TagValueSeriesIDIterator/78888_series_TagValueSeriesIDIterator/no_cache-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkIntegerArrayDecodeAllPackedSimple/1000-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkIntegerArrayDecodeAllPackedSimple/5-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkIntegerArrayDecodeAllPackedSimple/55-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkIntegerArrayDecodeAllPackedSimple/555-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkIntegerArray_IncludeFirst_10000-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkParsePointsTagsUnSorted5-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkValues_EncodeInteger-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkWritePoints_ExistingSeries_250K-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkWritePoints_NewSeries_100_Measurements_1_TagKey_1_TagValue-2...\n",
      "  Already analyzed, skip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-87164366bd4d>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmarkMeasurements.sort_values(by=[\"number\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv(filename,sep=\";\")\n",
    "\n",
    "\n",
    "# For each microbenchmark\n",
    "for name in df_all.name.unique():\n",
    "    print(f\"Running analysis for benchmark {name}...\")\n",
    "\n",
    "    benchmarkMeasurements = df_all.loc[(df_all['name'].str.startswith(name, na=False))]\n",
    "    benchmarkMeasurements.sort_values(by=[\"number\"], inplace=True)\n",
    "    instanceRuns = benchmarkMeasurements.instanceRun.unique()\n",
    "    suiteRuns = benchmarkMeasurements.suiteRun.unique()\n",
    "\n",
    "\n",
    "    if len(benchmarkMeasurements) > 50:\n",
    "        printName = name.replace(\"/\",\"-\")\n",
    "        printName = printName.replace(\"/\",\"-\")\n",
    "        printName = printName.replace(\"\\\\\",\"-\")\n",
    "        benchmarkFilename = os.path.join(outFolderName, printName + \".csv\")\n",
    "        if path.exists(benchmarkFilename):\n",
    "            print(\"  Already analyzed, skip.\")\n",
    "        else:\n",
    "            results = []\n",
    "            # For each commit (number)\n",
    "            lastValues = []\n",
    "\n",
    "            for commitNumber in benchmarkMeasurements.number.unique():\n",
    "                print(f\"  Running analysis for commit {commitNumber}...\")\n",
    "\n",
    "                #Find median perf. change\n",
    "                perfRuntimes1 = benchmarkMeasurements.loc[(benchmarkMeasurements['number'] == commitNumber)\n",
    "                                                    & (benchmarkMeasurements['version'] == 1)]\n",
    "                perfRuntimes2 = benchmarkMeasurements.loc[(benchmarkMeasurements['number'] == commitNumber)\n",
    "                                                    & (benchmarkMeasurements['version'] == 2)]\n",
    "\n",
    "\n",
    "                elements1 = perfRuntimes1['runtime'].shape[0]\n",
    "                elements2 = perfRuntimes2['runtime'].shape[0]\n",
    "                if (elements1 == 45) & (elements2 == 45):\n",
    "\n",
    "                    perf1 = perfRuntimes1['runtime'].median()\n",
    "                    perf2 = perfRuntimes2['runtime'].median()\n",
    "                    # Compare both (e.g., 10ms in ver1 and 12ms in ver2 => 12/10 = 1.2 (>1 -> regression)\n",
    "                    perfChange = ((perf2/perf1) - 1) * 100\n",
    "\n",
    "                    print(f\"    Found median performance change ({perfChange}).\")\n",
    "\n",
    "                    # Run Bootstrapping\n",
    "                    # R stores the 10.000 median values\n",
    "                    R = resample(perfRuntimes1=perfRuntimes1,\n",
    "                            perfRuntimes2=perfRuntimes2,\n",
    "                            instanceRuns=instanceRuns,\n",
    "                            suiteRuns=suiteRuns,\n",
    "                            numberOfIterations=numberOfIterations,\n",
    "                            numberOfSamples=numberOfSamples)\n",
    "\n",
    "                    print(f\"    Bootstrapping done ({len(R)} elements in R).\")\n",
    "\n",
    "                    # Find conf. intervals\n",
    "                    R.sort()\n",
    "\n",
    "                    small = int((numberOfSamples * CIsmall) / 100 / 2)\n",
    "                    if small == 0:\n",
    "                        small  = 1\n",
    "\n",
    "                    minSmall = R[small-1]\n",
    "                    minSmall = (minSmall - 1) * 100\n",
    "                    maxSmall = R[numberOfSamples-small-1]\n",
    "                    maxSmall = (maxSmall - 1) * 100\n",
    "                    print(f\"    Found conf. intervals: ([{minSmall}, {maxSmall}]).\")\n",
    "\n",
    "                    lastValues.append({\n",
    "                        \"commit\": commitNumber,\n",
    "                        \"min\": minSmall,\n",
    "                        \"med\": perfChange,\n",
    "                        \"max\": maxSmall,\n",
    "                    })\n",
    "\n",
    "                    # Jump detection\n",
    "                    jump = \"\"\n",
    "                    if (len(lastValues) > 1):\n",
    "                        currVal = lastValues[len(lastValues)-1]['med']\n",
    "                        prevVal = lastValues[len(lastValues)-2]['med']\n",
    "                        diff = currVal - prevVal\n",
    "                        #print(f\"diff is {diff}. {prevVal} -> {currVal}\")\n",
    "                        if (diff > jumpThreshold):\n",
    "                            jump = \"potential up\"\n",
    "                        if ((-1 * diff) > jumpThreshold):\n",
    "                            jump = \"potential down\"\n",
    "                        if (jump != \"\"):\n",
    "                            # check CIs\n",
    "                            currMin = lastValues[len(lastValues)-1]['min']\n",
    "                            currMax = lastValues[len(lastValues)-1]['max']\n",
    "                            prevMin = lastValues[len(lastValues)-2]['min']\n",
    "                            prevMax = lastValues[len(lastValues)-2]['max']\n",
    "                            if (currMin > prevMax):\n",
    "                                jump = \"definite up\"\n",
    "                            if (currMax < prevMin):\n",
    "                                jump = \"definite down\"\n",
    "\n",
    "                    if (jump != \"\"):\n",
    "                        print(f\"    Found {jump} jump at commit {commitNumber}.\")\n",
    "\n",
    "                    # Trend detection\n",
    "                    trend = \"\"\n",
    "                    #Clear values if there is a definite jump\n",
    "                    if (jump.startswith(\"definite\")):\n",
    "                        lastValues = lastValues[-1:]\n",
    "\n",
    "                    if (len(lastValues) > 2):\n",
    "                        currVal = lastValues[len(lastValues)-1]['med']\n",
    "                        sumOfPrevVals = 0\n",
    "                        for val in lastValues[:-1]:\n",
    "                            sumOfPrevVals += val['med']\n",
    "                        diff = currVal - (sumOfPrevVals / (len(lastValues)-1))\n",
    "                        #print(f\"diff is {diff}. {sumOfPrevVals / slidingAvg} -> {currVal}\")\n",
    "                        if (diff > trendThreshold):\n",
    "                            trend = \"potential up\"\n",
    "                        if ((-1 * diff) > trendThreshold):\n",
    "                            trend = \"potential down\"\n",
    "                        if (trend != \"\"):\n",
    "                            currMin = lastValues[len(lastValues)-1]['min']\n",
    "                            currMax = lastValues[len(lastValues)-1]['max']\n",
    "                            for val in lastValues[:-1]:\n",
    "                                prevMin = val['min']\n",
    "                                prevMax = val['max']\n",
    "                                if (currMin > prevMax):\n",
    "                                    trend = \"definite up\"\n",
    "                                if (currMax < prevMin):\n",
    "                                    trend = \"definite down\"\n",
    "\n",
    "\n",
    "                    if (trend != \"\"):\n",
    "                        print(f\"    Found {trend} trend at commit {commitNumber}.\")\n",
    "\n",
    "                    # Remove first element\n",
    "                    if (len(lastValues) > slidingAvg):\n",
    "                        lastValues.pop(0)\n",
    "\n",
    "                    #Store values\n",
    "                    results.append({\n",
    "                                    \"name\" : name,\n",
    "                                    \"number\" : int(commitNumber),\n",
    "                                    \"min\" : minSmall,\n",
    "                                    \"med\" : perfChange,\n",
    "                                    \"max\" : maxSmall,\n",
    "                                    \"jump\": jump,\n",
    "                                    \"trend\": trend\n",
    "                                    })\n",
    "                else:\n",
    "                    print(f\"  Skip {name};{commitNumber}, only ({elements1}, {elements2}) elements.\")\n",
    "\n",
    "            #Store results for this benchmark to file\n",
    "            # Convert to data frame\n",
    "            df_results = pd.DataFrame(results)\n",
    "            if len(results) > 0:\n",
    "                df_results.sort_values(by=[\"name\",\"number\"], inplace=True)\n",
    "                df_results.describe()\n",
    "\n",
    "                #Create folder\n",
    "                #Save file\n",
    "                df_results.to_csv(benchmarkFilename, sep=\";\")\n",
    "\n",
    "    else:\n",
    "        print(f\"  Skip {name}, only {len(benchmarkMeasurements)} measurments.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Done.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}