{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import seaborn as sns\n",
    "import os\n",
    "from os import path\n",
    "sns.set()\n",
    "rng = default_rng()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "numberOfSamples = 10000\n",
    "filename = \"../../results_all/micro_history_full_influxdb.csv\"\n",
    "outFolderName = \"../../results_all/micro_bootstrapping_influx\"\n",
    "\n",
    "#filename = \"../../results_all/micro_history_opti_influxdb.csv\"\n",
    "#outFolderName = \"../../results_all/micro_bootstrapping_influx_opti\"\n",
    "summaryFileName = \"summaryInflux.csv\"\n",
    "\n",
    "debug = True\n",
    "assertRelevances = False\n",
    "# iterations of microbenchmarks\n",
    "numberOfIterations = 5\n",
    "CIsmall = 1\n",
    "\n",
    "initialThreshold=6\n",
    "slidingAvg=3\n",
    "\n",
    "relevances = {\n",
    "    \"BenchmarkCreateIterator\":9.848,\n",
    "    \"BenchmarkDecodeFloatArrayBlock\":0.758,\n",
    "    \"BenchmarkIndex_IndexFile_TagValueSeriesIDIterator\":2.879,\n",
    "    \"BenchmarkIntegerArrayDecodeAllPackedSimple\":0.455,\n",
    "    \"BenchmarkWritePoints_NewSeries_100_Measurements_1_TagKey_1_TagValue\":11.818\n",
    "}\n",
    "\n",
    "relevances = {\n",
    "    \"BenchmarkCreateIterator\":1013.900,\n",
    "    \"BenchmarkDecodeFloatArrayBlock\":251.210,\n",
    "    \"BenchmarkIndex_IndexFile_TagValueSeriesIDIterator\":852.130,\n",
    "    \"BenchmarkIntegerArrayDecodeAllPackedSimple\":116.800,\n",
    "    \"BenchmarkWritePoints_NewSeries_100_Measurements_1_TagKey_1_TagValue\":942.970\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#! mkdir -p {outFolderName} # type: ignore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def resample(perfRuntimes1: pd.DataFrame,\n",
    "            perfRuntimes2: pd.DataFrame,\n",
    "            instanceRuns: np.ndarray,\n",
    "            suiteRuns: np.ndarray,\n",
    "            numberOfIterations: int,\n",
    "            numberOfSamples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Resamples performances using hierarchical bootstrapping for building confindence intervals\n",
    "\n",
    "        Builds a tensor of random indices of a form numberOfSamples * instanceRunsNumber * suiteRunsNumber * numberOfIterations.\n",
    "        Then uses these indices to choose from a performance runtimes tensor with a form instanceRunsNumber * suiteRunsNumber * numberOfIterations.\n",
    "        Finally, reshapes resulting tensor to a matrix of form numberOfSamples * ( instanceRunsNumber * suiteRunsNumber * numberOfIterations)\n",
    "        and calculates performance differences between medians.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfRuntimes1 : performance runtimes of the first version.\n",
    "        perfRuntimes2 : performance runtimes of the second version.\n",
    "        instanceRuns : array of instanceRun numbers\n",
    "        suiteRuns : array of suiteRun numbers\n",
    "        numberOfIterations : number of microbenchmark iterations\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            array of performance differences with a shape numberOfSample * 1.\n",
    "\n",
    "        \"\"\"\n",
    "    instanceRunsNumber = instanceRuns.shape[0]\n",
    "    suiteRunsNumber = suiteRuns.shape[0]\n",
    "\n",
    "    allRuntimes1 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "    allRuntimes2 = np.ndarray((instanceRunsNumber, suiteRunsNumber, numberOfIterations))\n",
    "\n",
    "    #aggregate measurements from all instance and suite runs\n",
    "    for instanceRun in instanceRuns:\n",
    "        for suiteRun in suiteRuns:\n",
    "            allRuntimes1[instanceRun - 1][suiteRun] = perfRuntimes1.loc[(perfRuntimes1['instanceRun'] == instanceRun)\n",
    "                                                    & (perfRuntimes1['suiteRun'] == suiteRun),\n",
    "                                                    'runtime'].to_numpy()\n",
    "            allRuntimes2[instanceRun - 1][suiteRun] = perfRuntimes2.loc[(perfRuntimes2['instanceRun'] == instanceRun)\n",
    "                                                    & (perfRuntimes2['suiteRun'] == suiteRun),\n",
    "                                                    'runtime'].to_numpy()\n",
    "    #Generate Random Arrays\n",
    "    currentInstanceRun = rng.choice(instanceRuns, size=(instanceRunsNumber, numberOfSamples)) - 1\n",
    "    currentSuiteRun = rng.choice(suiteRuns, size=(suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "    currentRuntimes1 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "    currentRuntimes2 = rng.integers(numberOfIterations, size=(numberOfIterations, suiteRunsNumber, instanceRunsNumber, numberOfSamples))\n",
    "\n",
    "    #Bulk selection\n",
    "    tmp1 = allRuntimes1[currentInstanceRun, currentSuiteRun, currentRuntimes1]\n",
    "    tmp1 = tmp1.reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "\n",
    "    tmp2 = allRuntimes2[currentInstanceRun, currentSuiteRun, currentRuntimes2]\n",
    "    tmp2 = tmp2.reshape((numberOfSamples, suiteRunsNumber * instanceRunsNumber * numberOfIterations))\n",
    "    # Get median for both lists\n",
    "    med1 = np.median(tmp1, axis=1)\n",
    "    med2 = np.median(tmp2, axis=1)\n",
    "\n",
    "    return med2/med1\n",
    "\n",
    "def bootstrap(perfRuntimes1: pd.DataFrame,\n",
    "            perfRuntimes2: pd.DataFrame):\n",
    "\n",
    "    instanceRuns = perfRuntimes1.instanceRun.unique()\n",
    "    numberOfInstanceRuns = len(instanceRuns)\n",
    "\n",
    "    suiteRuns = perfRuntimes1.suiteRun.unique()\n",
    "    numberOfSuiteRuns = len(suiteRuns)\n",
    "\n",
    "    print(f\"    Found {numberOfInstanceRuns} instance runs\")\n",
    "    R = resample(perfRuntimes1=perfRuntimes1,\n",
    "                 perfRuntimes2=perfRuntimes2,\n",
    "                 instanceRuns=instanceRuns,\n",
    "                 suiteRuns=suiteRuns,\n",
    "                 numberOfIterations=5,\n",
    "                 numberOfSamples=numberOfSamples)\n",
    "\n",
    "    print(f\"    Bootstrapping done ({len(R)} elements in R).\")\n",
    "\n",
    "    # Find conf. intervals\n",
    "    R.sort()\n",
    "\n",
    "    small = int((numberOfSamples * CIsmall) / 100 / 2)\n",
    "    if small == 0:\n",
    "        small  = 1\n",
    "\n",
    "    minSmall = R[small-1]\n",
    "    minSmall = (minSmall - 1) * 100\n",
    "    maxSmall = R[numberOfSamples-small-1]\n",
    "    maxSmall = (maxSmall - 1) * 100\n",
    "    return minSmall, maxSmall"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit 100: 109 microbenchmarks\n",
      "Commit 105: 109 microbenchmarks\n",
      "Commit 110: 109 microbenchmarks\n",
      "Commit 15: 426 microbenchmarks\n",
      "Commit 20: 426 microbenchmarks\n",
      "Commit 25: 426 microbenchmarks\n",
      "Commit 30: 426 microbenchmarks\n",
      "Commit 35: 426 microbenchmarks\n",
      "Commit 40: 426 microbenchmarks\n",
      "Commit 45: 426 microbenchmarks\n",
      "Commit 50: 426 microbenchmarks\n",
      "Commit 55: 424 microbenchmarks\n",
      "Commit 60: 414 microbenchmarks\n",
      "Commit 65: 414 microbenchmarks\n",
      "Commit 70: 414 microbenchmarks\n",
      "Commit 75: 414 microbenchmarks\n",
      "Commit 80: 414 microbenchmarks\n",
      "Commit 85: 412 microbenchmarks\n",
      "Commit 90: 109 microbenchmarks\n",
      "Commit 95: 109 microbenchmarks\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv(filename,sep=\";\")\n",
    "\n",
    "\n",
    "for commit in df_all.number.unique():\n",
    "    number = 0\n",
    "    tmp = df_all.loc[(df_all['number'] == commit)]\n",
    "    num = len(tmp.name.unique())\n",
    "    number += num\n",
    "    print(f\"Commit {commit}: {number} microbenchmarks\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis for benchmark BenchmarkAppendUnescaped-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBlockTypeToInfluxQLDataType-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBooleanArrayDecodeAll/1-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBooleanArrayDecodeAll/1000-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBooleanArrayDecodeAll/55-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBooleanArrayDecodeAll/555-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBooleanDecoder_DecodeAll/1-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBooleanDecoder_DecodeAll/1000-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBooleanDecoder_DecodeAll/55-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBooleanDecoder_DecodeAll/555-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBytesEscapeMany-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkBytesEscapeNoEscapes-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCacheParallelFloatEntries-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkContains_False-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkContains_True-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCountIterator_100K-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCountIterator_1K-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCountIterator_1M-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_1-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_2-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_3-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_4-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkCreateIterator/tsi1_shards_5-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecode-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeBlock_Boolean_Empty-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeBlock_Boolean_EqualSize-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeBlock_Boolean_TypeSpecific-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeBlock_Float_Empty-2...\n",
      "  Already analyzed, skip.\n",
      "Running analysis for benchmark BenchmarkDecodeBlock_Float_EqualSize-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-83c3ec4ff5fb>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmarkMeasurements.sort_values(by=[\"number\"], inplace=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-83c3ec4ff5fb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Running analysis for benchmark {name}...\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0mbenchmarkMeasurements\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_all\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_all\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m     \u001B[0mbenchmarkMeasurements\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msort_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mby\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"number\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0minstanceRuns\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbenchmarkMeasurements\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minstanceRun\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\BeFaaS-analysis\\lib\\site-packages\\pandas\\core\\strings.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1999\u001B[0m                 )\n\u001B[0;32m   2000\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2001\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2002\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2003\u001B[0m         \u001B[0mwrapper\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc_name\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\BeFaaS-analysis\\lib\\site-packages\\pandas\\core\\strings.py\u001B[0m in \u001B[0;36mwrapper3\u001B[1;34m(self, pat, na)\u001B[0m\n\u001B[0;32m   2050\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mforbid_nonstring_types\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mforbidden_types\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2051\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mwrapper3\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpat\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2052\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_parent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpat\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mna\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2053\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrap_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturns_string\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturns_string\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfill_value\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mna\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2054\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\BeFaaS-analysis\\lib\\site-packages\\pandas\\core\\strings.py\u001B[0m in \u001B[0;36mstr_startswith\u001B[1;34m(arr, pat, na)\u001B[0m\n\u001B[0;32m    511\u001B[0m     \"\"\"\n\u001B[0;32m    512\u001B[0m     \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 513\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_na_map\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbool\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    514\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\BeFaaS-analysis\\lib\\site-packages\\pandas\\core\\strings.py\u001B[0m in \u001B[0;36m_na_map\u001B[1;34m(f, arr, na_result, dtype)\u001B[0m\n\u001B[0;32m    128\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mna_result\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m         \u001B[0mna_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 130\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_map_object\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna_value\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mna_result\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    131\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\BeFaaS-analysis\\lib\\site-packages\\pandas\\core\\strings.py\u001B[0m in \u001B[0;36m_map_object\u001B[1;34m(f, arr, na_mask, na_value, dtype)\u001B[0m\n\u001B[0;32m    213\u001B[0m         \u001B[0mconvert\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    214\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 215\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_infer_mask\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muint8\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconvert\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    216\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    217\u001B[0m             \u001B[1;31m# Reraise the exception if callable `f` got wrong number of args.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer_mask\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\BeFaaS-analysis\\lib\\site-packages\\pandas\\core\\strings.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    510\u001B[0m     \u001B[0mdtype\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    511\u001B[0m     \"\"\"\n\u001B[1;32m--> 512\u001B[1;33m     \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    513\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0m_na_map\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbool\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    514\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "summary = []\n",
    "\n",
    "# For each microbenchmark\n",
    "for name in df_all.name.unique():\n",
    "    print(f\"Running analysis for benchmark {name}...\")\n",
    "\n",
    "    benchmarkMeasurements = df_all.loc[(df_all['name'].str.startswith(name, na=False))]\n",
    "    benchmarkMeasurements.sort_values(by=[\"number\"], inplace=True)\n",
    "    instanceRuns = benchmarkMeasurements.instanceRun.unique()\n",
    "    suiteRuns = benchmarkMeasurements.suiteRun.unique()\n",
    "\n",
    "\n",
    "    if len(benchmarkMeasurements) > 50:\n",
    "        printName = name.replace(\"/\",\"-\")\n",
    "        printName = printName.replace(\"/\",\"-\")\n",
    "        printName = printName.replace(\"\\\\\",\"-\")\n",
    "        benchmarkFilename = os.path.join(outFolderName, printName + \".csv\")\n",
    "        if path.exists(benchmarkFilename):\n",
    "            print(\"  Already analyzed, skip.\")\n",
    "        else:\n",
    "            results = []\n",
    "            lastValues = []\n",
    "            thresholds = []\n",
    "            thresholds.append(initialThreshold)\n",
    "            assert len(thresholds) > 0\n",
    "\n",
    "            for commitNumber in benchmarkMeasurements.number.unique():\n",
    "                print(f\"  Running analysis for commit {commitNumber}...\")\n",
    "\n",
    "                #Find median perf. change\n",
    "                perfRuntimes1 = benchmarkMeasurements.loc[(benchmarkMeasurements['number'] == commitNumber)\n",
    "                                                    & (benchmarkMeasurements['version'] == 1)]\n",
    "                perfRuntimes2 = benchmarkMeasurements.loc[(benchmarkMeasurements['number'] == commitNumber)\n",
    "                                                    & (benchmarkMeasurements['version'] == 2)]\n",
    "\n",
    "\n",
    "                elements1 = perfRuntimes1['runtime'].shape[0]\n",
    "                elements2 = perfRuntimes2['runtime'].shape[0]\n",
    "                if (elements1 == 45) & (elements2 == 45):\n",
    "\n",
    "                    perf1 = perfRuntimes1['runtime'].median()\n",
    "                    perf2 = perfRuntimes2['runtime'].median()\n",
    "                    # Compare both (e.g., 10ms in ver1 and 12ms in ver2 => 12/10 = 1.2 (>1 -> regression)\n",
    "                    change = ((perf2/perf1) - 1) * 100\n",
    "\n",
    "                    print(f\"    Found median performance change ({change}).\")\n",
    "\n",
    "                    # Run Bootstrapping\n",
    "                    min, max = bootstrap(perfRuntimes1, perfRuntimes2)\n",
    "                    instability = max - min\n",
    "                    print(f\"    Min: {min} Max: {max} (Width: {instability})\")\n",
    "\n",
    "                    assert max >= change\n",
    "                    assert change >= min\n",
    "\n",
    "                    lastValues.append({\n",
    "                        \"commit\":int(commitNumber),\n",
    "                        \"min\": min,\n",
    "                        \"med\": change,\n",
    "                        \"max\": max})\n",
    "                    thresholds.append(instability)\n",
    "\n",
    "                    if (len(thresholds) > slidingAvg):\n",
    "                        thresholds.pop(0)\n",
    "\n",
    "                    #Calculate sliding threshold\n",
    "                    threshold = 0\n",
    "                    if (len(thresholds) > 2):\n",
    "                        sumOfPrevThresholds = 0\n",
    "                        for val in thresholds:\n",
    "                            sumOfPrevThresholds += val\n",
    "                        threshold = (sumOfPrevThresholds / len(thresholds))  * 0.75\n",
    "\n",
    "                    if threshold < 1:\n",
    "                        threshold = 1\n",
    "\n",
    "                    # Jump detection\n",
    "                    jump = \"\"\n",
    "                    if (len(lastValues) > 1):\n",
    "                        currVal = lastValues[len(lastValues)-1]['med']\n",
    "                        prevVal = lastValues[len(lastValues)-2]['med']\n",
    "                        diff = currVal - prevVal\n",
    "                        #print(f\"diff is {diff}. {prevVal} -> {currVal}\")\n",
    "                        if (diff > threshold):\n",
    "                            jump = \"potential up\"\n",
    "                        if ((-1 * diff) > threshold):\n",
    "                            jump = \"potential down\"\n",
    "                        if (jump != \"\"):\n",
    "                            # check CIs\n",
    "                            currMin = lastValues[len(lastValues)-1]['min']\n",
    "                            currMax = lastValues[len(lastValues)-1]['max']\n",
    "                            prevMin = lastValues[len(lastValues)-2]['min']\n",
    "                            prevMax = lastValues[len(lastValues)-2]['max']\n",
    "                            if (currMin > prevMax):\n",
    "                                jump = \"definite up\"\n",
    "                            if (currMax < prevMin):\n",
    "                                jump = \"definite down\"\n",
    "\n",
    "                    if (jump != \"\"):\n",
    "                        print(f\"    Found {jump} jump at commit {commitNumber}.\")\n",
    "\n",
    "                    # Trend detection\n",
    "                    trend = \"\"\n",
    "                    #Clear values if there is a definite jump\n",
    "                    if (jump.startswith(\"definite\")):\n",
    "                        lastValues = lastValues[-1:]\n",
    "\n",
    "                    if (len(lastValues) > 2):\n",
    "                        currVal = lastValues[len(lastValues)-1]['med']\n",
    "                        sumOfPrevVals = 0\n",
    "                        for val in lastValues[:-1]:\n",
    "                            sumOfPrevVals += val['med']\n",
    "                        diff = currVal - (sumOfPrevVals / (len(lastValues)-1))\n",
    "                        #print(f\"diff is {diff}. {sumOfPrevVals / slidingAvg} -> {currVal}\")\n",
    "                        if (diff > threshold):\n",
    "                            trend = \"potential up\"\n",
    "                        if ((-1 * diff) > threshold):\n",
    "                            trend = \"potential down\"\n",
    "                        if (trend != \"\"):\n",
    "                            currMin = lastValues[len(lastValues)-1]['min']\n",
    "                            currMax = lastValues[len(lastValues)-1]['max']\n",
    "                            for val in lastValues[:-1]:\n",
    "                                prevMin = val['min']\n",
    "                                prevMax = val['max']\n",
    "                                if (currMin > prevMax):\n",
    "                                    trend = \"definite up\"\n",
    "                                if (currMax < prevMin):\n",
    "                                    trend = \"definite down\"\n",
    "\n",
    "\n",
    "                    if (trend != \"\"):\n",
    "                        print(f\"    Found {trend} trend at commit {commitNumber}.\")\n",
    "\n",
    "                    # Remove first element\n",
    "                    if (len(lastValues) > slidingAvg):\n",
    "                        lastValues.pop(0)\n",
    "\n",
    "                    #Store values\n",
    "                    results.append({\n",
    "                                    \"name\" : name,\n",
    "                                    \"commit\" : int(commitNumber),\n",
    "                                    \"min\" : min,\n",
    "                                    \"med\" : change,\n",
    "                                    \"max\" : max,\n",
    "                                    \"jump\": jump,\n",
    "                                    \"trend\": trend\n",
    "                                    })\n",
    "\n",
    "                    if (jump != \"\" or trend != \"\"):\n",
    "                        relevance = -1\n",
    "                        for n, r in relevances.items():\n",
    "                            if name.startswith(n):\n",
    "                                relevance = r\n",
    "                        if assertRelevances:\n",
    "                            assert relevance >= 0\n",
    "                        summary.append({\n",
    "                            \"name\" : name,\n",
    "                            \"commit\" : int(commitNumber),\n",
    "                            \"relevance\" : relevance,\n",
    "                            \"jump\": jump,\n",
    "                            \"trend\": trend\n",
    "                        })\n",
    "                else:\n",
    "                    print(f\"  Skip {name};{commitNumber}, only ({elements1}, {elements2}) elements.\")\n",
    "\n",
    "            #Store results for this benchmark to file\n",
    "            # Convert to data frame\n",
    "            df_results = pd.DataFrame(results)\n",
    "            if len(results) > 0:\n",
    "                df_results.sort_values(by=[\"name\",\"commit\"], inplace=True)\n",
    "                df_results.describe()\n",
    "\n",
    "                #Create folder\n",
    "                #Save file\n",
    "                df_results.to_csv(benchmarkFilename, sep=\";\")\n",
    "\n",
    "    else:\n",
    "        print(f\"  Skip {name}, only {len(benchmarkMeasurements)} measurments.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame(summary)\n",
    "if len(summary) > 0:\n",
    "    df_summary.sort_values(by=[\"commit\"], inplace=True)\n",
    "    df_summary.describe()\n",
    "\n",
    "    #Create folder\n",
    "    #Save file\n",
    "    df_summary.to_csv(outFolderName + \"/\" + summaryFileName, sep=\";\")\n",
    "\n",
    "print(\"Done.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}